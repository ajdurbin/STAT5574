---
title: "STAT5574 Report"
author: "Alexander Durbin"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
geometry: margin = 1in
fontsize: 12pt
abstract: "This paper is the capstone requirement for STAT5574 and documents my experience as an intern at Summit Consulting. It includes history and background of the company, my role and daily responsibilites, as well as the statistcal and technical knowledge I've gained from this experience. The reader recognizes that a non-disclosure agreement was required for this internship."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
library(VennDiagram)

```
# Summit Consulting

Summit Consulting is a statistical consulting firm founded in 2003 by Dr. Albert Lee. Dr. Lee's specializations in sampling techniques and econometric modeling prepared him for pursuing a career as a senior consultant at Bates White. As senior consultant, Dr. Lee was a critical component in Bates White's largest cases, the vitamin antitrust litigation and W.R. Grace asbestos litigation. After being exposed to litigation, Dr. Lee began broadened his focus from consulting statistician to also statistical expert witness. Located in Washington, D.C., Dr. Lee believed that his background in economics, sampling techniques, and litigation preparation would best serve governmental clients. Thus, being in walking distance the Department of Labor, Federal Housing Financing Agency, and other governmental agencies has been a priority since.

One of Summit's first clients was the Office of Management and Budget. Here Dr. Lee assisted then-credit policy analyst Anthony Curcio on credit subsidy model development. Working closely toegther, Dr. Lee identified Mr. Curcio's experience in needing to outsource statistical work, coupled with his background in federal credit modeling and advanced knowledge of federal budgets as an asset and possible ally of Summit Consulting. Dr. Lee gained a wealth of knowledge from Mr. Curcio on further opportunities for government contracts, other consulting opportunities, and preparing to be an expert witness. Their mutual interests in economics and consulting work formed a powerful working relationship; Mr. Curcio joined Summit Consulting as its second employee, additional founding member, and second principal in 2003.

Hereafter, Dr. Lee led a small team of economists and statisticians working on short-term government consulting contracts sourced by Mr. Curcio. However, Dr. Lee sought additional consistent long-term projects. He cultivated his client relationships with the Department of Labor and was able to source a long-term contract with the agency. They have been Summit's longest client and are often prioritized. Additionally, Summit established a group of employees that work on-site fulltime. This team primarily works onsite with regular offfice hours to always be available to the Department of Labor.  

While Summit was consistently growing and maintaining its focus as a governmental contractor, Dr. Lee was not satisfied with the lack of opportunities as a statistical expert witness. These opportunities presented themselves during the United States Housing Bubble burst in 2006. Summit consulting was then hired on the behalf of law firm Quinn Emanuel, as well as Freddie Mac and CoreLogic, to construct Automated Valuation Models with the goal of possibly pursuing legal action against financial institutions that provided the majority of residential loans and residential home appraisals. Dr. Lee then researched how to idenfiy predatory residential mortgage backed securities. These predatory loans were quantified as those with significant appraisal inflation. Working with other economic experts, Dr. Lee successfully derived regression models that predict appraisal inflation and has since been used as a statistical expert witness in many cases to discuss their validity.

<!-- This paragraph needs to discuss the jump from this initial work to what summit is doing today. things like trials taking years to get underway, massive hiring, expanding of departments, core litigation team, and further investors joining the company. That should be a very sufficient introductory section on summit. dont really say anything about the workplace or technology, i think that technology can have its own section in this paper. just say how the work was leveraged into further long term contracts and dr lee was able to establish himself as an export witness with shane thompson further following him. so this paragraph need not be long just a time jump from 2007-2016 and how trials take years to complete, we furtther focusing on expanding into other litigations. -->

This abundance in litigation work has thus been a core focus of Summit with the majority of cases taking years of analytical and legal work before completetion. Dr. Lee writes multiple reports on his regression models and defends them in court before the opposition's expert witness gives his rebuttle report. Summit then has on average two weeks to provide their own rebuttle report and any other analytics they feel necessary to include. After Dr. Lee provides his reports and testimony, it can take months before the trial is back in session or concluded.

In between trials, Dr. Lee and Mr. Curcio have sought to further grow Summit's client base while still adhearing to its core focuses. Summit has expanded to multiple departments with different expertise. In addition to the litigation directorate, other departments include federal credit modeling and forecasting and applied statistics and economics. These groups adhere not only to Summit's vision but most importantly to its founding principals' education and professional experiences. Summit has also partnered with multiple academic institutions to cultivate a working group of associate subject matter experts in addition to opportunities to become expert witnesses. The average employee at Summit holds an advanced degree in statistics, economics, or accounting. Summit promotes continuous learning and provides a yearly education budget for each employee for further education or certification and regularly hosts employee training in sampling methodologies, statistical software packages, and machine learning. 

# Litigation

Summit's Litigation directorate is decomposed into several working teams, research and review, quality control, and AVM. Each team has separate responsibilites, but do overlap some during trial periods. Research and Review have the primary goal to prepare Dr. Lee as a statistical expert witness. They are responsible for assisting Dr. Lee in trial preparation, including mock depositions, export reports, and rebuttle reports. The quality control team focuses on both quality of statistical analyses and quality of code. They provide all statistical figures and tables in expert reports. Summit is required to submit all code to the opposition during trial. QC is prioritized with ensuring that all code used to construct figures, statisitcs, and tables is exact and well documented, including log files, file paths, and any function declarations. They serve an important purpose at Summit, any code that is not exact is almost certainly used in the opposition's arguments and reports. The AVM team is the smallest team in Litigation at Summit. They entirely consist of statistians with advanced programming experience and expertise. Their main purpose is running the Automated Valuation Model derived by Dr. Lee. Other responsibilities of the AVM team include cross-validation, ad-hoc analyses, data cleaning, and the maintenance of Summit's Amazon Web Services infrastructure. While I was recruited as a member of the AVM team, I had several opportunities and responsibilities to work with both the R&R and QC teams.

My initial responsiblities as a member of the AVM team were learning the AWS infrastructure. The AVM team uses custom Amazon Machine Images that automatically load the datasets from S3 storage and code from CodeCommit. These images have RStudio Server and all relevant packages for the AVM installed. Since AWS is HIPAA complient, Summit is allowed to store data from Quinn Emanuel and other government clients on S3 and save more money than using onsite data storage and backup. Another benefit is the ability to specify the hardware requirements of your image. Jobs that take hours or days running on your laptop can take minutes when using more powerful hardware on AWS, and you are only charged for the computational time the job takes. This ability to upscale hardware requirements was most useful during trial rebuttle periods where running analyses as quickly as possible is necessary. A supplementary requirement for AWS was learning version control with `Git`. When updating the AVM code or other scripts, CodeCommit provides a central location to put all source code files, complete with documentation on all changes and the ability to restore previous versions of scripts if necessary. I felt that AWS provided a homogenous computing environment necessary for statistical collaboriation. 

After experimenting with AWS and becoming comfortable with the environment, I started to learn Summit's AVM and it was very intimidating at first. I thought myself proefficient in `R` before starting at Summit, but was not prepared for this level of software development. The AVM was a multi-step procedure, split across multiple files, used `S4` objects, and required the use of many packages I was unfamiliar with. It can be summarized as follows:

```{r, eval = FALSE}
1 Load subjects
2 Clean subjects 
3 Apply conversion dictionary
4 Sort by feature
5 For each subject
    Find comparables 
    if Insufficient comparables
     Remove subject
    else 
     Fit model
6 Cross-validate
```
The AVM was designed to be run in parallel using nested functions and run on AWS. Using the `function foo(...)` elipsis feature in `R`, all AVM options and variables are able to be propogated down the nested functions. This was our main means of running sensitivities, cross-validation, switch from mean to median-based calculations, bootstrapping, or verbose output.

As a member of the AVM team, I was required to understand the AVM completely, both for debugging purposes and in case an AVM team member was questioned during a deposition. While Dr. Lee had derived the statistical theory for the model, the AVM team were the ones responsible for its implimentation in `R` and need to be prepared in case they are required to appear in court. I took this opportunity and started a long-term project of internally documenting the AVM for Summit. I used Summit's custom `CSS` theme, `R Markdown`, and \LaTeX \ elements to create professional documentation. This documentation totalled over 90 pages and was as extensive as possible. It included a flow chart made with `TikZ` to best visualize the path a subject takes through the AVM with what information is added or removed from it after each function call. Also included are hardware requirements, uploading new subject and comparable data to AWS, nested function structure, comparison of log files, debugging procedures, and complete function descriptions with object types and return values. I was able to learn the AVM through the writing process and debugging. 

The average AVM workflow consisted of an often-failed first attempt of running new subject and comparable data and then reading log files to find the formatting bug. After the successful initial run, we change the comparable data and re-run the model. Results between these fits is often consistent. We then begin stress-testing with runs using the median, 1% and 10% exact confidence intervals, bootstrap confidence intervals using 100, 500, and 1000 samples, and *kfold* cross-validation. We then store this data on `S3` and alert another team member for peer replication. If repliciation results are different, we compare log files to ensure equivalent settings in each run. If the replication results are the same we alert the quality control group to begin constructing summary statistics for the expert reports. 

<!-- Robust regression with MM estimation? -->

# Methodology Working Group

To further improve the litigation directorate's core competencies, Summit's Methodology Working Group meets weekly to discuss statistical techniques and methods that can be of benefit or harm to the opposition. MWG has monthly topcis of focus with meekly meetings. Each month begins with an introductory presentation of the topic by the group leader. Those with background in the particular topic then give five minute presentations on their experience, knowledge of pros and cons, and how Summit can utilize this technique. After resources such as packages, literature, or tutorials are discussed, each member of MWG is tasked with finding a possible project or solution that the topic may be applied to. The next two weeks consist of group presentations discussing the problem, solution, and results. The final monthly meeting is a Summit-wide presentation with the most successful solutions. Topics of interest included spatial statistics and machine learning. 

MWG is also responsible for the Lit Throwdown competition. Litigation is split into two teams, plaintiff and defense, with the goal of reenacting a trial. Each team is further split into R&R, QC, and AVM, where it's required that each participant is not a member of their usual team. The teams are given a sample data set with the plaintiff's argument. It is then their responsibility to proivide expert service to their 'client', not only in statistical analyses, but also in legal recourse. The competition takes place over two weeks, with expert report due dates, rebuttle reports, and a mock trial with each group's expert witness and judged by Dr. Lee.


# Deep Dives (survey package, and the adverse sample)

Another group I participated in as an intern was Summit's Deep Dive group. Similar to the AVM team, the Deep Dive team was responsible for ad-hoc analyses during trial periods. We needed to be prepared to complete any analyses as quickly as possible. Such problems were unique to each case requested by the opposition or Dr. Lee. The opposition's arguments usually were accompanied by a random sample of subject data and the majority of our solutions involed extrapolation of the data and comparison to our overall results. 

This was my first experience with sampling statistics and an introduction to the `survey` package in `R`. We predominately used the regression and logistic regression models built in. An example is the following: DATA AND REG

An intersting problem we were unable to derive confident solution for was an adverse sampling problem. We were given two equally sized samples, extrapolated them, and were provided an additional sample. However, this sample had observation in one of original samples only. This made valid extrapolation difficult. This problem is best visualized with a Venn-diagram.

```{r eval = TRUE, echo = FALSE, fig.width=5, fig.height=3, fig.align='center'}
  p <- draw.triple.venn(214, 198, 32, 43, 19, 0, 0, c('Sample 1', 'Sample 2', 'Sample 3'), fill = c("blue", "red", "green"))
```

<!-- # Robust Regression -->

<!-- # Spatial Lag Regression -->

<!-- # Boosted Trees -->

<!-- # kfold Cross Validation -->

<!-- # Technical Skills -->

<!-- # Company Culture -->

<!-- # Overall Experience -->